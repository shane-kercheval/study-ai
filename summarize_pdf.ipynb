{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL = 'https://arxiv.org/pdf/2403.18802.pdf'\n",
    "URL = '~/Downloads/ud923-rosenblum-garfinkel-paper.pdf'\n",
    "\n",
    "# when extracting the text from the PDF, INCLUDE_AT and EXCLUDE_AT are used to determine where to\n",
    "# start and stop extracting text. For example, if INCLUDE_AT is 'Abstract' and EXCLUDE_AT is\n",
    "# 'Acknowledgements', then the text extraction will start at (and include) the first occurrence of\n",
    "# 'Abstract' and stop at (and Exclude) the first occurrence of 'Acknowledgements'.\n",
    "INCLUDE_AT = \"ABSTRACT\"\n",
    "# EXCLUDE_AT = \"Acknowledgements\"\n",
    "EXCLUDE_AT = \"9 C ONCLUSION\"\n",
    "\n",
    "# MODEL = 'gpt-3.5-turbo-0125'\n",
    "# MODEL = 'gpt-4-0125-preview'\n",
    "MODEL = 'gpt-4o-mini'\n",
    "SYSTEM_MESSAGE = 'You are an AI assistant that gives detailed and intuitive explanations.'\n",
    "MAX_TOKENS=None\n",
    "TEMPERATURE=0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from source.library.pdf import clean_text_from_pdf, extract_text_from_pdf\n",
    "from llm_workflow.openai import OpenAIChat, num_tokens, MODEL_COST_PER_TOKEN\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    return OpenAIChat(MODEL, SYSTEM_MESSAGE, max_tokens=MAX_TOKENS, temperature=TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/Downloads/ud923-rosenblum-garfinkel-paper.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# download and extract text of pdf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mextract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mURL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m num_tokens(model_name\u001b[38;5;241m=\u001b[39mMODEL, value\u001b[38;5;241m=\u001b[39mtext)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# of tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_tokens\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/repos/study-ai/source/library/pdf.py:26\u001b[0m, in \u001b[0;36mextract_text_from_pdf\u001b[0;34m(pdf_path, delete_afterwards)\u001b[0m\n\u001b[1;32m     24\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mPdfReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_file_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m reader\u001b[38;5;241m.\u001b[39mpages:\n\u001b[1;32m     28\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text()\n",
      "File \u001b[0;32m~/repos/study-ai/env/lib/python3.12/site-packages/pypdf/_reader.py:133\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[0;34m(self, stream, strict, password)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_id2num: Optional[Dict[Any, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validated_root: Optional[DictionaryObject] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_encryption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encryption: Optional[Encryption] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/study-ai/env/lib/python3.12/site-packages/pypdf/_reader.py:151\u001b[0m, in \u001b[0;36mPdfReader._initialize_stream\u001b[0;34m(self, stream)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fh:\n\u001b[1;32m    152\u001b[0m         stream \u001b[38;5;241m=\u001b[39m BytesIO(fh\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/Downloads/ud923-rosenblum-garfinkel-paper.pdf'"
     ]
    }
   ],
   "source": [
    "# download and extract text of pdf\n",
    "text = extract_text_from_pdf(pdf_path=URL)\n",
    "n_tokens = num_tokens(model_name=MODEL, value=text)\n",
    "print(f\"# of tokens: {n_tokens:,}\")\n",
    "print(f\"Cost if input tokens == {n_tokens:,}:  ${MODEL_COST_PER_TOKEN[MODEL]['input'] * n_tokens:.3f}\")\n",
    "print(f\"Cost if output tokens == {n_tokens:,}: ${MODEL_COST_PER_TOKEN[MODEL]['output'] * n_tokens:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191,545 characters before\n",
      "32,612 characters after\n",
      "Removed 82.97% of text\n",
      "Preview:\n",
      "---\n",
      "\n",
      "ABSTRACT\n",
      "\n",
      "Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate\n",
      "\n",
      "LongFact, a prompt set comprising thousands of questions spanning 38 topics.\n",
      "\n",
      "We then propose that LLM agents can be used as automated evaluators for long- form factuality through a method which we call Search-Augmented Factuality\n",
      "\n",
      "Evaluator (SAFE). SAFE\n"
     ]
    }
   ],
   "source": [
    "# removed text before `INCLUDE_AT` and after `EXCLUDE_AT`\n",
    "chars_before = len(text)\n",
    "print(f\"{chars_before:,} characters before\")\n",
    "text = clean_text_from_pdf(text=text, include_at=INCLUDE_AT, exclude_at=EXCLUDE_AT)\n",
    "chars_after = len(text)\n",
    "print(f\"{chars_after:,} characters after\")\n",
    "print(f\"Removed {abs((chars_after - chars_before) / chars_before):.2%} of text\")\n",
    "print(\"Preview:\\n---\\n\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens: 7,906\n",
      "Input cost if input tokens == 7,906:   $0.079\n",
      "Output cost if output tokens == 7,906: $0.237\n"
     ]
    }
   ],
   "source": [
    "n_tokens = num_tokens(model_name=MODEL, value=text)\n",
    "print(f\"# of tokens: {n_tokens:,}\")\n",
    "print(f\"Input cost if input tokens == {n_tokens:,}:   ${MODEL_COST_PER_TOKEN[MODEL]['input'] * n_tokens:.3f}\")\n",
    "print(f\"Output cost if output tokens == {n_tokens:,}: ${MODEL_COST_PER_TOKEN[MODEL]['output'] * n_tokens:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: $0.102\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Simplified Paper Overview**\n",
       "\n",
       "**Abstract Simplified**\n",
       "We created a new way to test if large language models (like GPT-4) can provide accurate, long answers across a variety of topics. We made a big list of questions (called LongFact) covering 38 topics. Then, we introduced a new method (named SAFE) that uses a language model to check if the answers are true by searching the web. We also came up with a scoring system that considers both the number of correct facts and the desired length of an answer. Our tests showed that this method is better and much cheaper than asking people to check the answers. We also found that bigger language models tend to give more accurate long answers.\n",
       "\n",
       "**Introduction Simplified**\n",
       "Even though large language models have gotten really good, they often make mistakes when giving detailed answers. They might get dates wrong or make up things about well-known people. We aim to improve how we test these models for accuracy by introducing a new set of questions (LongFact), a checking method (SAFE), and a scoring system that looks at answer accuracy and length. Our tests show that our method works well and is much cheaper than using people to check answers.\n",
       "\n",
       "**LongFact Simplified**\n",
       "There aren't many good tests for seeing if language models can provide long, accurate answers on a wide range of topics. So, we made LongFact by asking GPT-4 to come up with lots of questions that need detailed answers. These questions cover 38 different topics. This is the first big test of its kind for checking detailed answer accuracy across many subjects.\n",
       "\n",
       "**SAFE Simplified**\n",
       "Judging the accuracy of long answers is tricky. We suggest focusing on checking each fact in an answer separately and using web searches to see if they're true. Our method, SAFE, breaks down answers into facts, decides if they're relevant, and then checks them online. This approach helps us accurately judge if a detailed answer is true or not.\n",
       "\n",
       "**Comparing SAFE to Human Checkers Simplified**\n",
       "When we compared our SAFE method to answers checked by humans, SAFE agreed with the human checkers 72% of the time. In cases where they disagreed, SAFE was right 76% of the time. Also, SAFE was more than 20 times less expensive than using human checkers. This shows that our method not only works well but can also save a lot of money.\n",
       "\n",
       "**F1@K: A New Scoring System Simplified**\n",
       "We want detailed answers to be both accurate and comprehensive. Our new scoring system, F1@K, helps us measure both by considering how many facts in an answer are correct and how the length of the answer matches what we're looking for. This helps us better understand how well a model can provide detailed, accurate information.\n",
       "\n",
       "**Testing Larger Language Models Simplified**\n",
       "We tested 13 different language models to see how well they could provide detailed, accurate answers using our new methods. Bigger models generally did better, giving more accurate and comprehensive answers. This suggests that as models get larger, they might also become more reliable in giving long, detailed answers.\n",
       "\n",
       "**Related Work and Limitations Simplified**\n",
       "Other studies have tried to test models' accuracy with short answers or on specific topics. Our work aims to check accuracy in long, detailed answers across many topics. While our method is a big improvement, it relies on the ability of language models to follow instructions and on the assumption that web searches can always provide the right facts to check an answer. There might also be cases where Google Search doesn’t have all the answers, so there's still room for improvement."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model()\n",
    "prompt = f\"\"\"\n",
    "Rewrite this entire paper and each individual section using concise, simple, and intuitive language. Remove unnecessary jargon but keep relevant and important terminology. Write at least 5-7 sentences for each section. Keep all important details, insights, and key points, while removing or simplifying immaterial details. The point of view should still be from the authors' and should contain all relevant information, but it should be much easier to understand.\n",
    "\n",
    "Here is the paper:\n",
    "\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "response = model(prompt)\n",
    "with open('summary.txt', 'w') as f:\n",
    "    f.write(response)\n",
    "cost = model.cost\n",
    "print(f\"Cost: ${cost:.3f}\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cost:            $0.10213\n",
      "Total Tokens:          8,753\n",
      "Total Prompt Tokens:   8,023\n",
      "Total Response Tokens: 730\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Cost:            ${model.cost:.5f}\")\n",
    "print(f\"Total Tokens:          {model.total_tokens:,}\")\n",
    "print(f\"Total Prompt Tokens:   {model.input_tokens:,}\")\n",
    "print(f\"Total Response Tokens: {model.response_tokens:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Do the authors discuss the specifics or different scenarios of when not to use cosine similarity?\"\n",
    "response = model(prompt)\n",
    "cost = model.cost\n",
    "print(f\"Cost: ${cost:.3f}\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
